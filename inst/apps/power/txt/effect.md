### How can we measure the effect size?

Imagine, you are examining the mean difference of two (experimental) groups A and B, as the density plot on the left side illustrates. The effect size is determined by the *mean difference* between A and B and the *standard deviation* of both groups. The overlap between the distribution of A and B becomes smaller if the mean difference between and A and B increases. However, this depends also on the standard deviation of both distributions. As the standard distribution increases, the overlap between A and B also increases. Give it a try and adjust the values for the mean differences and the standard deviation.

So, how can measure the effect size if it depend on how we measure the variables? Cohen's δ is a very popular measure for effect sizes and it helps to get rid of the underlying scale. In order to estimate Cohen's δ we take in principal the estimated difference in the means of both groups and divide it by the pooled standard deviation of X and Y. Cohen also provided some guidance to assess whether the effect of X on Y is small, medium, or large:

| Effect size   | Cohen (1988) |
|---------------|--------------|
| Small effect  | δ = 0,2      |
| Medium effect | δ = 0,5      |
| Large effect  | δ = 0,8      |

Thus, measuring effect size is useful to compare the results of different studies since the effect size no longer depends on the measured outcome. Moreover, how we measure effect sizes depends also on the applied method. We can estimate the effect size in terms of correlation as the last pane showed, but we will apply a power analysis for two experimental groups. In order to understand what power analysis does, it is sufficient to know that we can transfer results from one approach to another. For instance, we can convert a correlation coefficient to δ. Depending on the effect we hope to detect, we may have a lower or a higher statistical power to detect an effect if it really exists.
